{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7d579c-0aba-4478-9682-b80ebe985c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9107571-9e03-4b85-b3b4-4911938357f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 17101/17101 [00:17<00:00, 984.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5406/5406 [00:04<00:00, 1135.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 12167\n",
      "Validation samples: 3546\n",
      "Classes found: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "train_img_dir = r\"E:\\YOLO_format\\train\\images\"\n",
    "train_label_dir = r\"E:\\YOLO_format\\train\\labels\"\n",
    "\n",
    "val_img_dir = r\"E:\\YOLO_format\\valid\\images\"\n",
    "val_label_dir= r\"E:\\YOLO_format\\valid\\labels\"\n",
    "\n",
    "def load_affectnet_labels(img_dir, label_dir):\n",
    "    img_paths = []\n",
    "    labels = []\n",
    "    for file in tqdm(os.listdir(label_dir)):\n",
    "        if file.endswith(\".txt\"):\n",
    "            label_file = os.path.join(label_dir, file)\n",
    "            image_file = os.path.join(img_dir, file.replace(\".txt\", \".jpg\"))\n",
    "            if os.path.exists(image_file):\n",
    "                with open(label_file, \"r\") as f:\n",
    "                    parts = f.readline().strip().split()\n",
    "                    if len(parts) > 0:\n",
    "                        expr = int(parts[0])\n",
    "                        if 0 <= expr <= 7:   # valid emotion labels\n",
    "                            img_paths.append(image_file)\n",
    "                            labels.append(expr)\n",
    "    return img_paths, np.array(labels)\n",
    "\n",
    "train_paths, train_labels = load_affectnet_labels(train_img_dir, train_label_dir)\n",
    "val_paths, val_labels = load_affectnet_labels(val_img_dir, val_label_dir)\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "print(f\"Training samples: {len(train_paths)}\")\n",
    "print(f\"Validation samples: {len(val_paths)}\")\n",
    "print(f\"Classes found: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e816404b-8316-4c06-b311-21046fc21fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH = 8\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_image(img_path, label):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "def augment(img, label):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_brightness(img, 0.15)\n",
    "    img = tf.image.random_contrast(img, 0.85, 1.15)\n",
    "    img = tf.image.random_saturation(img, 0.9, 1.1)\n",
    "    return img, label\n",
    "\n",
    "def build_dataset(file_paths, labels, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(len(file_paths))\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = build_dataset(train_paths, train_labels, training=True)\n",
    "val_ds = build_dataset(val_paths, val_labels, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1bb384-325a-439e-aa78-b37eebeb1c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.7506786771964462, 1: 1.0467136958017893, 2: 0.8154825737265415, 3: 0.7912981269510926, 4: 4.650993883792049, 5: 2.044186827956989, 6: 0.9613621997471555, 7: 0.6765458185053381}\n"
     ]
    }
   ],
   "source": [
    "class_weights_arr = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_arr))\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5887b85e-7d63-4f79-a3ec-d6ad5a8d6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               327936    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,587,976\n",
      "Trainable params: 329,992\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(num_classes, input_shape=(224,224,3)):\n",
    "    base = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.35)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_model(num_classes)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360a9d6a-0739-4e04-8fad-840cfc68c555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1521/1521 [==============================] - 46s 28ms/step - loss: 2.0119 - accuracy: 0.2176 - val_loss: 1.8882 - val_accuracy: 0.2702 - lr: 0.0010\n",
      "Epoch 2/8\n",
      "1521/1521 [==============================] - 42s 28ms/step - loss: 1.8852 - accuracy: 0.2691 - val_loss: 1.8257 - val_accuracy: 0.3077 - lr: 0.0010\n",
      "Epoch 3/8\n",
      "1521/1521 [==============================] - 42s 28ms/step - loss: 1.8455 - accuracy: 0.2841 - val_loss: 1.7358 - val_accuracy: 0.3407 - lr: 0.0010\n",
      "Epoch 4/8\n",
      "1521/1521 [==============================] - 42s 27ms/step - loss: 1.8172 - accuracy: 0.2932 - val_loss: 1.8498 - val_accuracy: 0.2597 - lr: 0.0010\n",
      "Epoch 5/8\n",
      "1521/1521 [==============================] - 42s 27ms/step - loss: 1.7965 - accuracy: 0.2920 - val_loss: 1.7671 - val_accuracy: 0.3111 - lr: 0.0010\n",
      "Epoch 6/8\n",
      "1521/1521 [==============================] - 42s 28ms/step - loss: 1.7876 - accuracy: 0.2997 - val_loss: 1.7158 - val_accuracy: 0.3579 - lr: 0.0010\n",
      "Epoch 7/8\n",
      "1521/1521 [==============================] - 42s 28ms/step - loss: 1.7619 - accuracy: 0.3035 - val_loss: 1.7124 - val_accuracy: 0.3373 - lr: 0.0010\n",
      "Epoch 8/8\n",
      "1521/1521 [==============================] - 42s 27ms/step - loss: 1.7393 - accuracy: 0.3117 - val_loss: 1.7614 - val_accuracy: 0.3017 - lr: 0.0010\n",
      "Epoch 1/15\n",
      "1521/1521 [==============================] - 122s 76ms/step - loss: 1.6857 - accuracy: 0.3370 - val_loss: 1.6123 - val_accuracy: 0.3880 - lr: 1.0000e-05\n",
      "Epoch 2/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 1.5166 - accuracy: 0.3919 - val_loss: 1.4387 - val_accuracy: 0.4422 - lr: 1.0000e-05\n",
      "Epoch 3/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 1.4200 - accuracy: 0.4261 - val_loss: 1.4288 - val_accuracy: 0.4349 - lr: 1.0000e-05\n",
      "Epoch 4/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 1.3450 - accuracy: 0.4547 - val_loss: 1.3695 - val_accuracy: 0.4729 - lr: 1.0000e-05\n",
      "Epoch 5/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 1.2738 - accuracy: 0.4768 - val_loss: 1.3688 - val_accuracy: 0.4848 - lr: 1.0000e-05\n",
      "Epoch 6/15\n",
      "1521/1521 [==============================] - 116s 76ms/step - loss: 1.2033 - accuracy: 0.5043 - val_loss: 1.2151 - val_accuracy: 0.5282 - lr: 1.0000e-05\n",
      "Epoch 7/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 1.1442 - accuracy: 0.5240 - val_loss: 1.2044 - val_accuracy: 0.5268 - lr: 1.0000e-05\n",
      "Epoch 8/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 1.0839 - accuracy: 0.5428 - val_loss: 1.2697 - val_accuracy: 0.5296 - lr: 1.0000e-05\n",
      "Epoch 9/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 1.0268 - accuracy: 0.5740 - val_loss: 1.1550 - val_accuracy: 0.5595 - lr: 1.0000e-05\n",
      "Epoch 10/15\n",
      "1521/1521 [==============================] - 113s 74ms/step - loss: 0.9733 - accuracy: 0.5918 - val_loss: 1.1868 - val_accuracy: 0.5578 - lr: 1.0000e-05\n",
      "Epoch 11/15\n",
      "1521/1521 [==============================] - 115s 75ms/step - loss: 0.9181 - accuracy: 0.6152 - val_loss: 1.0780 - val_accuracy: 0.5956 - lr: 1.0000e-05\n",
      "Epoch 12/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 0.8673 - accuracy: 0.6378 - val_loss: 1.0698 - val_accuracy: 0.5936 - lr: 1.0000e-05\n",
      "Epoch 13/15\n",
      "1521/1521 [==============================] - 114s 75ms/step - loss: 0.8253 - accuracy: 0.6621 - val_loss: 1.1242 - val_accuracy: 0.5894 - lr: 1.0000e-05\n",
      "Epoch 14/15\n",
      "1521/1521 [==============================] - 115s 75ms/step - loss: 0.7909 - accuracy: 0.6725 - val_loss: 1.0191 - val_accuracy: 0.6156 - lr: 1.0000e-05\n",
      "Epoch 15/15\n",
      "1521/1521 [==============================] - 115s 75ms/step - loss: 0.7364 - accuracy: 0.6893 - val_loss: 1.0139 - val_accuracy: 0.6213 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.keras.callbacks.ModelCheckpoint(\"affectnet_best.h5\", monitor='val_accuracy', save_best_only=True)\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Train only the classification head first\n",
    "history1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=8,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[ckpt, early, reduce]\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "model.get_layer('mobilenetv2_1.00_224').trainable = True\n",
    "for layer in model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[ckpt, early, reduce]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d3980a5-9fbe-469a-85d0-c5208b12986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd578f9aab4947e7a798a9f45324feb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268596b529054e42ae58227d52ab2ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class_names = [\"neutral\", \"happy\", \"sad\", \"surprise\", \"fear\", \"disgust\", \"anger\", \"contempt\"]\n",
    "\n",
    "# Creating upload widget\n",
    "uploader = widgets.FileUpload(accept=\"image/*\", multiple=True)\n",
    "out = widgets.Output()\n",
    "display(uploader)\n",
    "display(out)\n",
    "display(uploader)\n",
    "\n",
    "# Handler to run prediction directly after upload\n",
    "def on_upload_change(change):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        for item in change.new:\n",
    "            # Read the image bytes\n",
    "            img_bytes = item.content\n",
    "            img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "            # Preprocess image\n",
    "            img = img.resize(IMG_SIZE)\n",
    "            img_array = np.array(img) / 255.0\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "            # Run model prediction\n",
    "            preds = model.predict(img_array)\n",
    "            top_idx = np.argmax(preds[0])\n",
    "            confidence = preds[0][top_idx] * 100\n",
    "\n",
    "            # Display result\n",
    "            print(f\" {item.name}\")\n",
    "            print(f\"Predicted Emotion: {class_names[top_idx]} ({confidence:.2f}%)\")\n",
    "\n",
    "            plt.figure(figsize=(7, 4))\n",
    "            plt.bar(class_names, preds[0]*100)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel(\"Probability (%)\")\n",
    "            plt.title(f\"Prediction for {item.name}\")\n",
    "            plt.show()\n",
    "\n",
    "# Attach the handler to the widget\n",
    "uploader.observe(on_upload_change, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fef06cd-8644-4d50-b5df-6287a2acc3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted: contempt (65.38%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES = [\"neutral\", \"happy\", \"sad\", \"surprise\", \"fear\", \"disgust\", \"anger\", \"contempt\"]\n",
    "\n",
    "def predict_emotion(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = img.resize(IMG_SIZE)\n",
    "    arr = np.array(img)/255.0\n",
    "    arr = np.expand_dims(arr, 0)\n",
    "    pred = model.predict(arr)[0]\n",
    "    top_idx = np.argmax(pred)\n",
    "    print(f\"Predicted: {CLASS_NAMES[top_idx]} ({pred[top_idx]*100:.2f}%)\")\n",
    "    return top_idx\n",
    "\n",
    "predict_emotion(val_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ea4c1-6fce-4f3b-8ec9-e99cf6ba1ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
